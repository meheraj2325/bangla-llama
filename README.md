# Bangla-llama

This repository contains the code and models for <b>Bangla-Llama</b>," a project aimed at enhancing the performance of language models for the Bangla language. It builds upon the open-source LLaMA model using the LoRA methodology for efficient training. Many of the scripts are adapted from the <a href="https://github.com/abhinand5/tamil-llama"> tamil-llama </a> repository, with additional inspiration from the <a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/tree/main">Chinese-LLaMA-Alpaca-2 </a> repository. Necessary modifications have been made to tailor the training process for Bangla-Llama on a large Bangla corpus of approximately 12 GB, sourced from the Bangla 2B+ BERT dataset, one of the largest datasets available for the Bangla language.

# Available Models
<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Type</th>
      <th>Data</th>
      <th>Base Model</th>
      <th>#Params</th>
      <th>Download Links</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Bangla-Llama</td>
      <td>Base Model</td>
      <td>12 GB</td>
      <td>LLaMA 7B</td>
      <td>7B</td>
      <td><a href="https://huggingface.co/meherajj/bangla-llama2-7b-orig-llama-tokenizer">HF HUB</a></td>
    </tr>
  </tbody>
</table>
